
<font size=3><div align='center'>  
[[📖 arXiv Paper](#)] 
[[📊 LRS-VQA Dataset](#)] 
[[🛠️ Implementation Code](#)] 

</div></font>

## When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning

**[2025/3/11]** 🔥 **LRS-VQA** is now released! Code and weight will be released soon.


This project focuses on  perception capabilities of Large Vision-Language Models (LVLMs) in the context of Large Remote Sensing Images (RSIs), covering the following key aspects:

- **Region Focus Module (RFM):** Learns text-aware key vision token localization capabilities through attention distillation, enabling focused analysis on critical image tiles.
- **Coarse-to-fine text-guided token pruning with Dynamic Image Pyramid (DIP):** Enhances both accuracy and efficiency in high-resolution settings.
- **LRS-VQA:** A new benchmark for Large RSI perception, featuring 7,333 QA pairs across 8 categories, with images reaching up to 27,328 pixels in length.

---

## 🛠️ **Methodology: Coarse-to-Fine Token Pruning**

### Core Components

#### 1. **Dynamic Image Pyramid (DIP)**


#### 2. **Region Focus Module (RFM)**


#### 3. **Token Pruning Strategy**


---

## 📚 **LRS-VQA Dataset**

